‘Big data’ is getting even bigger due to an explosion in computing devices and the amount of time users spend online. Much of this data is widely distributed due to the global presence of major search and social networking platforms. For example, the data generated and stored by Google’s internal infrastructure, as well as user data collected through its search and advertising services are spread over its multiple datacenters. There are examples of highly physically distributed systems such as sensor and cellular networks where communication links are expensive. On such networks,  backhauling data back to a central server to perform analytics tasks would be slow and impractical. This becomes challenging when much of this data needs to be processed and analyzed in real-time.   

MapReduce is a parallel programming model widely used to analyze large data sets. Traditional MapReduce models assume data stored and jobs scheduled in a single cluster, and is not suitable for an heterogeneous environment where the input data is very large and distributed among multiple clusters. Traditional MapReduce models rely on a centralized master node, through which data has to transit to be scheduled to the worker nodes. Backhauling all the data to a central location quickly becomes slow and expensive over a wide-area network because bandwidth is a scarce resource.

When processing large amounts of physically distributed data, users may prefer a “quick and dirty” result over a correct answer that could take much longer to compute \cite{3, 4}. To this end, we wish to develop a MapReduce infrastructure for data processing on wide-area networks that minimizes communication on expensive links at the expense of result correctness. We explore two ways to reduce data exchange on expensive links between physically distributed nodes: (1) performing the map-reduce jobs locally on each data-center while periodically updating results across data centers, and (2) by reducing the size of the updates sent across data-centers. The first approach could affect convergence and correctness of the results if updates are not frequent enough (see PageRank). The second approach is applicable to a restricted set of queries discussed below (see Top-K).