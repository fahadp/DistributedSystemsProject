\section{Introduction}

‘Big data’ is getting even bigger due to explosive growth in computing devices, infrastructure and the amount of time users spend online. There's huge value to analyzing this data in a timely manner. For example, fast analytics over machine logs in a datacenter, or phone records collected at cell towers, will help the operator find and debug the root cause of performance problems quickly. Much of this data is widely distributed due to the global presence of major cellular and cloud service providers. For example, the data generated and stored by Google’s internal infrastructure, as well as user data collected through its search and advertising services are spread over hundreds of datacenters spread throughout the world. Similarly a cellular provider's switching and serving infrastructure in a single metropolitan area alone consists of hundreds of base stations, each collecting gigabytes of data The principal bottleneck in transferring and analyzing data over inter-cluster links in such systems is the available bandwidth. Therefore backhauling data to a central location for analytics purposes would be significantly slow as well as expensive. 

We present Meteor, a system designed for fast analytics over data spread over multiple clusters and possibly  
multiple filesystems. Meteor is built on top of the Spark cluster computing framework, and uses a two-pronged 
approach. First, it implements location awareness and opportunistic work-stealing at the scheduler level. Second, it explores the the use of approximate analytics at the application level for iterative queries.

MapReduce is a parallel programming model widely used to analyze large data sets. Traditional MapReduce models assume data stored and jobs scheduled in a single cluster, and is not suitable for an heterogeneous environment where the input data is distributed among multiple clusters. Traditional MapReduce models rely on a shared filesystem, such as HDFS, accessible to all nodes. Although a shared filesystem provides universal access and reliability properties, backhauling and replicating data from geographically dispersed locations to a shared filesystem cannot be achieved in real-time since bandwidth is a scarce resource. 
Additionally the map and reduce phases rely on all-to-all communication between nodes in a cluster.
This maybe reasonable in a tightly coupled cluster with full bisection bandwidth, but is an unrealistic assumption in our highly distributed setting where bandwidth is scarce. 

When processing large amounts of physically distributed data, users may prefer a “quick and dirty” result over a correct answer that could take much longer to compute \cite{blinkdb, hop}. To this end, we develop a MapReduce infrastructure for analytics on wide-area networks that minimizes communication on expensive links at the expense of result correctness. We explore two ways to reduce data exchange on expensive links between physically distributed nodes: (1) performing the map phase locally at each node, and (2) by reducing the size of shuffle output sent across nodes. 
%The first approach could affect convergence and correctness of the results if updates are not frequent enough (see PageRank). The second approach is applicable to a restricted set of queries discussed below (see Top-K).