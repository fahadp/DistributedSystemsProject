\section{Introduction}

‘Big data’ is getting even bigger due to explosive growth in computing devices, infrastructure and the amount of time users spend online. There's huge value to analyzing this data in a timely manner. For example, fast analytics over machine logs in a datacenter, or phone records collected at cell towers, will help the operator find and debug the root cause of performance problems quickly. 

Much of this data is widely distributed due to the global presence of major cellular and cloud service providers. For example, the data generated and stored by Google’s internal infrastructure, as well as user data collected through its search and advertising services are spread over hundreds of data centers spread throughout the world. Similarly a cellular provider's switching and serving infrastructure in a single metropolitan area alone consists of hundreds of base stations, each collecting gigabytes of data. The principal bottleneck in transferring and analyzing data over inter-cluster links in such systems is the available bandwidth. Not only is the raw capacity on these links limited, most of this capacity is used for background, bulk transfers for replication \cite{greenberg}. \cite{greenberg} further reports that the cost of deploying this wide-area infrastructure is much more than the cost of networking equipment within a single data center. Therefore backhauling data from every datacenter to a central location for on-the-fly analytics is significantly slow and expensive.

We present Meteor, a system designed to fulfill the need for fast analytics across multiple data centers. We use the term 'data center' loosely here that reflects a highly distributed environment with significant bandwidth limitations between either individual nodes or clusters of nodes. Each node or groups of node may have its own local storage or share a networked storage with the rest of the system. We build Meteor on top of the Spark cluster computing framework \cite{rdd}. Spark offers a general execution model for in-memory computing which lets query data faster than disk-based engines like Hadoop. 

We use a two-pronged approach to achieve our goal of providing fast analytics across multiple data centers. First, we modify the Spark scheduler to add data locality and bandwidth aware work-stealing. Traditional MapReduce-like frameworks assume a tightly coupled cluster with a shared filesystem equally accessible from all nodes. Therefore they are not suited to a heterogeneous environment where the input data is distributed among multiple clusters, possibly on separate filesystems, and limited inter-cluster bandwidth makes timely replication nearly impossible. Meteor addresses these concerns by exposing a richer API such that the application is able to specify co-location of data with compute nodes. The scheduler then enforces these constraints by scheduling map tasks for the co-located data only on the specified hosts, instead in a round-robin fashion. In addition to data locality, Meteor implements an adaptive bandwidth-aware work stealing policy as part of the Spark scheduler. Although strict data locality already significantly improves the responsiveness of the system, data skew likely at different locations will result in the cluster with the smallest data split remaining idle for a major part of the execution. To take advantage of idle processor resources, the scheduler lets an idle host request and operate on a remote data partition with a user-defined probability $p$. Meteor makes this work-stealing bandwidth-aware by continuously measuring the available bandwidth on all links, and sending the remote block only for the highest bandwidth link. 

Second, we explore the potential of approximate iterative queries at reducing communication bandwidth on expensive nodes the application level. When processing large amounts of physically distributed data, users may prefer a “quick and dirty” result over a correct answer that could take much longer to compute \cite{blinkdb, hop}. To this end, we extend Spark's MapReduce framework to provide approximate iterative analytics on wide-area networks that minimizes communication on expensive links at the expense of result correctness. Iterative map-reduce jobs may require all-to-all communication between each iteration, leading to expensive bandwidth requirements on links that route messages between two geographically distant nodes. We wish to explore how running interative jobs by partitioning the data and computing it in isolation, while performing periodic updates can affect result correctness. 