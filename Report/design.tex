\section{Design}
\label{sec: design}

To achieve fast analytics of massive datasets spread over the wide area, we have implemented several key components as part of Meteor, as described below.

\subsection{Location-Aware Map}

\begin{figure}[!ht]
\centering\includegraphics[width=0.5\columnwidth]{figs/lam.pdf}
\vspace{-1.2em}
\caption{Scheduler keeps separate task queues for each cluster/host in location-aware map.}
\label{fig:lam}
\vspace{.7em}
\end{figure}

The system should reduce reliance on network bandwidth since it's a scarce resource. To prevent backhauling
all the data from every location to a centralized location at the beginning of the job, input data should remain in place. To prevent map inputs shuttled over the wire, we implement Location-Aware Map (LAM). LAM scheduling achieves data locality by essentially forbidding a node executing a map task over a data partition residing in a remote cluster. Conceptually LAM scheduling is shown in Figure \ref{fig:lam}. Instead of a FIFO queue of tasks, the scheduler has a separate task queue for each cluster $n$. Therefore a node in cluster $n$ executes a map task that
belonged to its own queue. We provide an API call in Spark to let the programmer attach each data source, i.e. RDD, to a set of nodes. The scheduler puts each map task for partitions for this data source in the appropriate queue. It is interesting to note that Location-Aware Map is equivalent to Delay-Scheduling \cite{delay-scheduling} with infinite delay.
We show in \S \ref{sec:eval}  that LAM provides significant latency and bandwidth improvements over vanilla Spark/MapReduce. 

\subsection{Bandwidth-Aware Work Stealing}

Datasets in the wild are not split equally; in fact, there's is a lot of skew in the size of geographically dispersed datasets.  With strict data locality, this will result in some set of clusters rendered idle after finishing their local computation, and hence opportunities for parallelism will go amiss.  Careful scheduling of tasks on remote nodes in this case could lead to even better runtime results. To this end, we implement an adaptive work-stealing policy based on the measured bandwidth between clusters as part of the Spark scheduler. We call this Bandwidth-Aware Work Stealing (BAWS). BAWS scheduling is shown in Figure \ref{fig:baws}. BAWS is implemented on top of LAM. During the LAM stage, all nodes operate in normal. When a node $n$'s task queue gets empty,
instead of remaining idle, it tries to steal a task off a remote node's queue with probability $p$. $p$ is configurable by the user. This work-stealing is bandwidth-aware since the candidate node to steal from is the one that the stealer has the highest measured bandwidth at that time. Therefore BAWS is adaptive to network conditions. 
We show in \S \ref{sec:eval}  that BAWS improves LAM scheduling in terms of job latency.

\begin{figure}[!ht]
\centering\includegraphics[width=0.5\columnwidth]{figs/BOSS.pdf}
\vspace{-1.2em}
\caption{As part of Bandwidth-Aware Work Stealing, Meteor constantly measures the available bandwidth between each pair of clusters. Work Stealing only kicks in when a node's task queue is empty. It then steals a remote node's task with the highest bandwidth with probability $p$. }
\label{fig:baws}
\vspace{.7em}
\end{figure}

\subsection{Communication-avoiding iterative MapReduce}

There has been a great deal of research on improving iterative MapReduce jobs \cite{haloop, twister, rdd}. by maintaining intermediate data in memory, and consistently partitioning data across iterations. We explore in details how these optimizations help reduce communication in the case of PageRank in section \ref{sec:pagerank}.

Further to these techniques, combiners help reduce inter-node communication \cite{hop}. Combiner work in the same way as reduce functions, but are used in intermediate stages of a MapReduce job in order to summarize input values it was passed for a given key. Combiners essentially perform aggreagation on the map side in order to reduce the amount of network traffic required between map and reduce phases.  
