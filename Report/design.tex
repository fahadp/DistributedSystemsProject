\section{Design}
\label{sec: design}

To achieve fast analytics of massive datasets spread over the wide area, we have implemented several key components as part of Meteor, as described below.

\subsection{Location-Aware Map}

\begin{figure}[!ht]
\centering\includegraphics[width=0.5\columnwidth]{figs/lam.pdf}
\vspace{-1.2em}
\caption{Scheduler keeps separate task queues for each cluster/host in location-aware map.}
\label{fig:lam}
\vspace{.7em}
\end{figure}

The system should reduce reliance on network bandwidth since it's a scarce resource. To prevent backhauling
all the data from every location to a centralized location at the beginning of the job, input data should remain in place. To prevent map inputs shuttled over the wire, we implement Location-Aware Map (LAM). LAM scheduling achieves data locality by essentially forbidding a node executing a map task over a data partition residing in a remote cluster. Conceptually LAM scheduling is shown in Figure \ref{fig:lam}. Instead of a FIFO queue of tasks, the scheduler has a separate task queue for each cluster $n$. Therefore a node in cluster $n$ executes a map task that
belonged to its own queue. We provide an API call in Spark to let the programmer attach each data source, i.e. RDD, to a set of nodes. The scheduler puts each map task for partitions for this data source in the appropriate queue. It is interesting to note that Location-Aware Map is equivalent to Delay-Scheduling \cite{delay-scheduling} with infinite delay.
We show in \S \ref{sec:eval}  that LAM significant latency and bandwidth improvements over vanilla Spark/MapReduce. 

\subsection{Bandwidth-Aware Work Stealing}

Datasets in the wild are split equally; in fact, there's is a lot of skew in the size of geographically dispersed datasets.  With strict data locality, this will result in some set of clusters rendered idle after finishing their local computation, and hence opportunities for parallelism will go amiss.  Careful scheduling of tasks on remote nodes in this case could lead to even better runtime results. To this end, we implement an adaptive work-stealing policy based on the measured bandwidth between clusters as part of the Spark scheduler. We call this Bandwidth-Aware Work Stealing (BAWS). 

\begin{figure}[!ht]
\centering\includegraphics[width=0.5\columnwidth]{figs/BOSS.pdf}
\vspace{-1.2em}
\caption{As part of Bandwidth-Aware Work Stealing, Meteor constantly measures the available bandwidth between each pair of clusters. Work Stealing only kicks in when a node's task queue is empty. It then steals a remote node's task with the highest bandwidth with probability $p$. }
\label{fig:lam}
\vspace{.7em}
\end{figure}
